\documentclass[11pt, legalpaper]{article}
\usepackage[a4paper, left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{titlefoot}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{tikz}

% cool ass lighting bolts
\usepackage{marvosym}
\let\marvosymLightning\Lightning
\usepackage{wasysym}
\let\wasysymLightning\lightning
\usepackage{stmaryrd}
\let\stmaryrdLightning\lightning


\usetikzlibrary{positioning}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}





\title{Week 7 - Matrix analysis}
\author{Michel Cancalon - 325002}
\date{\today}

\begin{document}
\maketitle

\section{Exercise 1}
\begin{enumerate}
    \item Let's study the simple case where each couple produces 4 children.
    \begin{itemize}
        \item Couple AA - AA : 4 children with genotype AA
        \item Couple Aa - Aa : 1 child with genotype AA, 2 children with genotype Aa, 1 child with genotype aa
        \item Couple aa - aa : 4 children with genotype aa
    \end{itemize}
    As such we can see that the next generation will have:
    \begin{itemize}
        \item 5 children with genotype AA: 4 from the AA population and 1 from the Aa population
        \item 2 children with genotype Aa: 2 from the Aa population
        \item 5 children with genotype aa: 4 from the aa population and 1 from the Aa population
    \end{itemize}
    This leads us to the following matrix which represents the population increase after one generation:
    \begin{center}
        $\begin{pmatrix}
            4 & 1 & 0 \\
            0 & 2 & 0 \\
            0 & 1 & 4
        \end{pmatrix}$
    \end{center}
    However, we want to know the population proportion after one generation. As such we need to normalize the matrix so that given $\alpha_n + \beta_n + \gamma_n = 1$, we must have the sum of each population of the next generation also equal to 1. This leads us to the following calculation:
    $$\frac{1}{N}\cdot \begin{pmatrix}
        4 & 1 & 0 \\
        0 & 2 & 0 \\
        0 & 1 & 4
    \end{pmatrix} \cdot \begin{pmatrix} \alpha_n \\ \beta_n \\ \gamma_n \end{pmatrix}= \frac{1}{N} \cdot \begin{pmatrix} 4\alpha_n+\beta_n \\ 2\beta_n \\ \beta_n + 4\gamma_n \end{pmatrix} \Longleftrightarrow \frac{1}{N}(4\alpha_n+\beta_n +  2\beta_n + \beta_n + 4\gamma_n)=1 \Longleftrightarrow N=4 $$
    As such, the normalized matrix is:
    $$\boxed{M=\begin{pmatrix}
            1 & 0.25 & 0 \\
            0 & 0.5 & 0 \\
            0 & 0.25 & 1
        \end{pmatrix}}$$
    
    \item Given the recursive formula $x_n=Mx_{n-1}$, for n generations, we have:
        $$\boxed{x_n=M^nx_{0}}$$
    \item Let's first find the eignevalues of the matrix M:
    \begin{align*}\text{det}(M-\lambda I) & =\begin{vmatrix}
        1-\lambda & \frac{1}{4} & 0 \\
        0 & \frac{1}{2}-\lambda & 0 \\
        0 & \frac{1}{4} & 1-\lambda
    \end{vmatrix}\\
    &=(1-\lambda)^2\left(\frac{1}{2}-\lambda\right)\\ \end{align*}
    We find $\lambda_1=1$ and $\lambda_2=\frac{1}{2}$.\\
    Let's now find the eigenvectors associated with these eigenvalues:
    \begin{itemize}
        \item For $\lambda_1=1$:
        \begin{align*}
            (M-I)v_1 & =0\\
            \begin{pmatrix}
                0 & \frac{1}{4} & 0 \\
                0 & -\frac{1}{2} & 0 \\
                0 & \frac{1}{4} & 0
            \end{pmatrix}v_1 & =0
        \end{align*}
        We find $v_1=\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$ and $v_2=\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$
        \item For $\lambda_2=\frac{1}{2}$:
        \begin{align*}
            \left(M-\frac{1}{2}I\right)v_3 & =0\\
            \begin{pmatrix}
                \frac{1}{2} & \frac{1}{4} & 0 \\
                0 & 0 & 0 \\
                0 & \frac{1}{4} & \frac{1}{2}
            \end{pmatrix}v_3 & =0
        \end{align*}
        We find $v_3=\begin{pmatrix} -1 \\ 2 \\ -1 \end{pmatrix}$
    \end{itemize}

    \item We first notice that if we diagonalize the matrix by writting $M=PDP^{-1}$, we have:
    $$x_n=PDP^{-1}PDP^{-1}PDP^{-1}...PDP^{-1}x_0=PD^nP^{-1}x_0$$
    Let's diagonalize the matrix M:
    $$P=\begin{pmatrix}
        1 & 0 & -1 \\
        0 & 0 & 2 \\
        0 & 1 & -1
    \end{pmatrix} \Rightarrow  P^{-1}= \begin{pmatrix}
        1 & 0.5 & 0 \\
        0 & 0.5 & 1 \\
        0 & 0.5 & 0
    \end{pmatrix}$$
    As such, we have:
    $$D=P^{-1}M P=\begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 0.5
    \end{pmatrix}$$
    We can now easily calculate $x_n$:
    \begin{align*}
        x_n  &=PD^nP^{-1}x_0\\
                &=\begin{pmatrix}
        1 & 0 & -1 \\
        0 & 0 & 2 \\
        0 & 1 & -1
    \end{pmatrix}\begin{pmatrix}
        1^n & 0 & 0 \\
        0 & 1^n & 0 \\
        0 & 0 & 0.5^n
    \end{pmatrix}\begin{pmatrix}
        1 & 0.5 & 0 \\
        0 & 0.5 & 1 \\
        0 & 0.5 & 0
    \end{pmatrix}x_0\\
    &=\begin{pmatrix}
        1 & 0.5\cdot(1-0.5^n) & 0 \\
        0 & 0.5^n & 0 \\
        0 & 0.5\cdot(1-0.5^n) & 1
    \end{pmatrix} x_0
    \end{align*}
    For the case where $n=10$, we have:
    $$\boxed{x_{10}=\begin{pmatrix}
        1 & 0.5\cdot(1-0.5^{10}) & 0 \\
        0 & 0.5^{10} & 0 \\
        0 & 0.5\cdot(1-0.5^{10}) & 1
    \end{pmatrix} \begin{pmatrix} 1/8 \\ 3/4 \\ 1/8 \end{pmatrix}=\begin{pmatrix}\frac{1}{8}(1+3\frac{1023}{1024}) \\ \frac{3}{4\cdot 1024} \\ \frac{1}{8}(1+3\frac{1023}{1024})\end{pmatrix}=\begin{pmatrix}0.4996337890625 \\ 0.000732421875 \\ 0.4996337890625\end{pmatrix}}$$

    \item As $n$ goes to infinity, we have:
    $$\lim_{n\to \infty}x_n=\lim_{n\to \infty}\begin{pmatrix}
        1 & 0.5\cdot(1-0.5^n) & 0 \\
        0 & 0.5^n & 0 \\
        0 & 0.5\cdot(1-0.5^n) & 1
    \end{pmatrix}\begin{pmatrix} 1/8 \\ 3/4 \\ 1/8 \end{pmatrix}=\begin{pmatrix} 1 & 0.5 & 0 \\
        0 & 0& 0 \\
        0 & 0.5 & 1\end{pmatrix}\begin{pmatrix} 1/8 \\ 3/4 \\ 1/8 \end{pmatrix}=\begin{pmatrix} 0.5 \\ 0 \\ 0.5 \end{pmatrix}$$
    This makes sense because we saw that at the next generation, population $\beta$ decreased while $\alpha$ and $\gamma$ increased equally.
    
    \item We have the approximated equation:
    \begin{align*}
        &x_n-x_{n-1}=Qx_n\\
        \Longleftrightarrow & \\
        &(I-Q)x_n=x_{n-1}\\
        \Longleftrightarrow & \\
        &x_n=(I-Q)^{-1}x_{n-1} \\
        \Longleftrightarrow & \\
        &Mx_{n-1}=(I-Q)^{-1}x_{n-1}\\
        \Longleftrightarrow & \\
        &M=(I-Q)^{-1}
    \end{align*}
    So the relationship between the matrix $M$ and the matrix $Q$ is $ \boxed{M=(I-Q)^{-1}}$.\\
    Now, we can express write the eigen formula for $M$:
    \begin{align*}
        &Mv=\lambda v\\
        \Longleftrightarrow & \\
        &(I-Q)^{-1}v=\lambda v\\
        \Longleftrightarrow & \\
        &(I-Q)v=\frac{1}{\lambda}v\\
        \Longleftrightarrow& \\
        &v-Qv=\frac{1}{\lambda}v\\
        \Longleftrightarrow& \\
        &Qv=\frac{\lambda-1}{\lambda}v
    \end{align*}
    So we see that if $\lambda$ and $v$ are the eigenthingies of $M$, then $Q$ has the same eigenvector $v$ and has an eigenvalue of $\frac{\lambda-1}{\lambda}$.
    
    
\end{enumerate}


\section{Exercise 2}
\begin{enumerate}
    \item Given $A$ with elements $a_{i,j}$ and the matrix $D$ with elements $d_{i,j}$.\\
    Let's first write $L=D-A$, we see that $L$ has the form:
    $$l_{i,j}=\begin{cases}
        d_{i,i} = \text{Amount of edges connected to node $i$} & \text{if } i=j\\
        -a_{i,j}=-1 & \text{if } i \ne j \text{ and there is an edge between node $i$ and $j$}\\
        -a_{i,j}=0 & \text{if } i \ne j \text{ and there is no edge between node $i$ and $j$}
    \end{cases}$$
    Now let's consider $\tilde{L}=BB^T$, it's element $\tilde{l}_{i,j}$ is given by:
    \begin{equation}\tilde{l}_{i,j}=\sum_{m=1}^{|E|}b_{i,m}b_{j,m}\end{equation}
    For the matrix $B$, we see that each element $b_{i,j}$ with $i \in \{1, 2, ...,|V|\}$ and $j \in \{1, 2, ...,|E|\}$ describes whether the node $i$ in relationship to a edge $j$ is:
    \begin{itemize}
        \item Not connected: 0
        \item Connected and on the ass of the arrow: -1
        \item Connected and on the pointy bit of the arrow: 1
    \end{itemize}
    So, if we sum the absolute value of all elements of a line $i$ in the matrix $B$, we get the total amount of edges connected to the node $i$.\\
    Looking at equation $(1)$, we see that this is exactly what the elements of the diagonal achieves: 
    $$\tilde{l}_{i,i}=\sum_{m=1}^{|E|}b_{i,m}b_{i,m}=\text{Amount of edges connected node $i$} $$
    Let's now consider elements $\tilde{l}_{i,j}$ with $i \ne j$ (those who are not on the diagonal). \\
    We can see that $b_{i,m}b_{j,m}$ represents the statements:
    \begin{itemize}
        \item Node $i$ and $j$ are not connected through edge $m$: 0
        \item Node $i$ goes into $j$ or vice versa, through edge $m$: -1
    \end{itemize} 
    ($b_{i,m}b_{j,m} \ne 1$ because a edge can not be bidirectional) \\
    Now if we sum $b_{i,m}b_{j,m}$ i.e we calculate $\tilde{l}_{i,j}=\sum_{m=1}^{|E|}b_{i,m}b_{j,m}$, we see that $\tilde{l}_{i,j} \in \{-1, 0\}$ because for two nodes $i$ and $j$, there can be at most one edge that passes between them. \\
    So we conclude that:
    $$\tilde{l}_{i,j}=\begin{cases}
        \text{Amount of edges connected to node $i$} & \text{if } i=j\\
        -1 & \text{if } i \ne j \text{ and there is an edge between node $i$ and $j$}\\
        0 & \text{if } i \ne j \text{ and there is no edge between node $i$ and $j$}
    \end{cases}$$

    We see that the expressions of the elements of $L$ and $\tilde{L}$ match so $BB^T=D-A$.
    \item Let's first show that $\lambda_1=0$ is an eigenvalue of $L$. \\
            We know that for such a eigenvalue, the eigenvectors must satisfy the equation
            $$Lv=\lambda_1 v_1 \Longleftrightarrow (D-A)v_1=0$$ 
            Let's consider elements $a_{i,j}$ of the matrix $A$, we know that $a_{i,j}=1$ if there is an edge between node $i$ and $j$ and $a_{i,j}=0$ otherwise. 
            This means that if we sum all elements along line $i$ in the matrix $A$, we get the total amount of edges connected to the node $i$. \\
            Now lets consider the elements $d_{i,i}$ of the matrix $D$, we know that $d_{i,i}$ is the total amount of edges connected to the node $i$. \\
            As such we deduce that if we sum all elements $l_{i,j}=d_{i,j}-a_{i,j}$ along a line $i$ in the matrix $L$, the result is 0. \\
            Summing all elements of along a line of $L$ corresponds to the dot product of the line with a vector of ones. This means we respect the following equation:
            $$(D-A)\begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix}=\begin{pmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix}$$
            This means we have found that the vector of ones is an eigenvector of $L$ associated with the eigenvalue $\lambda_1=0$.\\

            Given that $L= BB^T$, the following also holds true:
            $$\sqrt{\lambda_1 L}=\sqrt{0 \cdot B B^T}=0$$
            This is exactly the definition of a singular value of $B$, so we have shown that $\lambda_1=0$ is also a singular value of $B$. 
    \item   We know that $L$ is of form:
            $$L=\begin{pmatrix}
                n-1 & -1 & \cdots & -1 \\
                -1 & n-1 & \cdots & -1 \\
                \vdots & \vdots & \ddots & \vdots \\
                -1 & -1 & \cdots & n-1
            \end{pmatrix}$$
            Given that $L$ is symetric by constrution, we know that it's eigenvectors are orthogonal to the one associated to $\lambda_1=0$. \\
            This means that if $\lambda_i$ ($i \ne 0$) is another eigenvalue with eigenvector $v_i$, we have:
            \begin{equation}
                \sum_{j=1}^{n}v_{i,j}v_{1,j}=\sum_{j=1}^{n}v_{i,j}=0
            \end{equation}
            Now let's develop the eigenvalue equation $Lv_i=\lambda_iv_i$ for $\lambda_i$:
            \begin{align*}
                \begin{pmatrix}
                    n-1 & -1 & \cdots & -1 \\
                    -1 & n-1 & \cdots & -1 \\
                    \vdots & \vdots & \ddots & \vdots \\
                    -1 & -1 & \cdots & n-1
                \end{pmatrix}\begin{pmatrix}
                    v_{i,1} \\ v_{i,2} \\ \vdots \\ v_{i,n}
                \end{pmatrix} & =\lambda_i \begin{pmatrix}
                    v_{i,1} \\ v_{i,2} \\ \vdots \\ v_{i,n}
                \end{pmatrix} \\
                \Longleftrightarrow & \\
                \begin{pmatrix}
                    v_{i,1}(n-1)-\sum_{j=2}^{n}v_{i,j} \\ v_{i,2}(n-1)-\sum_{j=1, j\ne 2}^{n}v_{i,j} \\ \vdots \\ v_{i,n}(n-1)-\sum_{j=1}^{n-1}v_{i,j}
                \end{pmatrix} & =\begin{pmatrix}
                    \lambda_i v_{i,1} \\\lambda_i  v_{i,2} \\ \vdots \\ \lambda_i v_{i,n}
                \end{pmatrix} \\
            \end{align*}
            We see that for a line $k$ in the matrix equation, we have:
            \begin{align*}
                &v_{i,k}(n-1)-\sum_{j=1, j\ne k}^{n}v_{i,j}=\lambda_i v_{i,k}\\
                \Longleftrightarrow &\\
                &v_{i,k}(n-1)-v_{i,k}(\lambda_i-1) =\sum_{j=1}^{n}v_{i,j}\\
                \Longleftrightarrow &\\
                &v_{i,k}(n-\lambda_i)=\sum_{j=1}^{n}v_{i,j}
            \end{align*}
            Using equation $(2)$ and the above, we can find that:
            $$\sum_{j=1}^{n}v_{i,j}=0 \Rightarrow v_{i,k}(n-\lambda_i)=0 \Rightarrow \lambda_i=n$$
            Given we chose an arbitrary line $k$ of the matrix equation, this result holds for all $k$ and we have shown that all eigenvalues (other than $0$) of $L$ are $n$.

            \item Let's compute this directly:
            \begin{align*}
                x^TLx &= x^TBB^Tx\\
                &= (B^Tx)^T(B^Tx)\\
                &= ||B^Tx||^2 \\
                &= \sum_{i=1}^{|E|}(B^Tx)_i^2\\
                &= \sum_{i=1}^{|E|}\left(\sum_{j=1}^{|V|}b_{j,i}x_j\right)^2\\
            \end{align*}
            We know that for a fixed column $i$, the vector $(b_{j,i})_{j=1,2,... |V|}$ tells us what nodes are connected at either end of the edge $i$. \\
            As such, $(b_{j,i})_{j=1,2,... |V|}$ is composed of one value $1$, one value $-1$ and the rest is zeroes. \\
            This means that the sum $\sum_{j=1}^{|V|}b_{j,i}x_j$ is the difference between two values $x_m$ and $x_n$ and who both correspond to nodes who are connected with edge $j$. \\
            So we can write:
            $$\sum_{j=1}^{|V|}b_{j,i}x_j=(x_{m}-x_{n})$$
            And so we conclude that:
            $$x^TLx=\sum_{i=1}^{|E|}\left(\sum_{j=1}^{|V|}b_{j,i}x_j\right)^2=\sum_{(m,n) \in E}(x_{m}-x_{n})^2$$
            Where $(m,n)$ is a pair of nodes connected by an edge amongst all edges $E$.
            \item Let's consider $1_S$ to be a "mask" of the nodes we want to include with shape $|V| \times 1$.\\
            Looking at $B$ (with size $|V| \times |E|$), we can see that each of it's colums corresponds to a particular edge.\\
            If we do the scalar product of $1_S$ with a single column of $B$ (which corresponds to an edge), we can get the following results:
            \begin{itemize}
                \item If the edge has both nodes in the mask $1_S$, the scalar product is $-1 \cdot 1 +1 \cdot 1=0$
                \item If the edge has only one node in the mask $1_S$, the scalar product is $-1$ or $1$
                \item If the edge has no nodes in the mask $1_S$, the scalar product is $0$
            \end{itemize}
            So if we sum up the absolute value of this scalar product for all edges (columns of $B$), we'll get $cut(S)$. This is done by doing the following operation:
            $$\boxed{1_S^TB \cdot B^T 1_S=1_S^T \cdot L \cdot1_S}$$
            In the above formula $B^T \cdot 1_S$ returns a column vector where each component is the scalar product described above, and multiplying this by $(B^T \cdot 1_S)^T=1_S^T B$ returns the sum of the absolute values of all the scalar products.

            \item If $S$ is a non empty subset of $V$, then $cut(S)=0$ implies that $S$ is a set of nodes that are not connected to any other nodes outside of $S$. This set of nodes basically forms an "island" in the graph. \\
            For example, the second graph that was given as an example in the exercise statement has indeed two islands of nodes which respect $cut(\{u,v,w\})=0$ and $cut(\{z,t\})=0$.

            \item The proof is laughably (dare I say trivially even) simple. Using what we have proved in the point 4. , that is:
            $$x^T L x = \sum_{(i,j)\in E} (x_i-x_j)^2$$
            We immediately see that a sum of squares is always positive or null, so $x^T L x \geq 0$. \\
            Hence, we have proved that $L$ is a positive semi-definite matrix.

            \item Let's consider an individual connected component of the graph which we'll call $L_i$.\\
            We know from point 2. that $L_i$ has an eigenvalue of $0$ which has a multiplicity of 1 at least. \\
            This means that we can write: $$det(L_i-\lambda I)=\lambda^{m_i} \cdot p_i$$ Where $p_i$ is a polynomial and $m_i \geq 1$ is the multiplicity of the eigenvalue $0$.\\
            Now, if we consider all the connected components of the graph, we can write the determinant of the Laplacian matrix as:
            $$det(L-\lambda I)=\prod_{i=1}^{k}det(L_i-\lambda I)=\prod_{i=1}^{k}\lambda^{m_i} \cdot p_i=\lambda^{\sum_{i=1}^{k}m_i} \cdot \prod_{i=1}^{k}p_i$$
            We see that the multiplicity of the eigenvalues $0$ is at the very least equal to the number $k$ of connected components of the graph (in the case where $m_i=1$ for all $i$).
            
            \item Let's suppose by contradiction that $\lambda_1=0$ has a multiplicity of 2 (or more) which means that it has two orthogonal (because $L$ is symetric) eigenvectors $v_1$ and $v_2$, $v_1$ being the vector of ones (defined in point 2.) and $v_2$ being another eigenvector orthogonal to $v_1$.\\
            We have $v_2 \ne \alpha \cdot v_1$ with $\alpha$ a scalar, because otherwise, both vectors would not be orthogonal. This means that $v_2$ can not be a constant vector.\\
            We know that $v_2$ is an eigenvector of $L$ associated with the eigenvalue $0$, so we can write:
            $$v_2^T L v_2=v_2^T \lambda_1 v_2=v_2^T 0 v_2=0$$
            Thanks to point 4. we can write:
            $$v_2^T L v_2=\sum_{(i,j)\in E} (v_{2,i}-v_{2,j})^2$$
            So we have:
            $$\sum_{(i,j)\in E} (v_{2,i}-v_{2,j})^2=0$$
            This means that for every edge $(i,j)$, to have $v_2^TL v_2=0$, we must have $v_{2,i}=v_{2,j}$.\\
            This equality extends to all nodes $v_i$ with $i \in V$ because the graph is connected (has no islands), so the same value must propagate through all the nodes of the graph.\\
            This means that $v_2$ is a constant vector, which contradicts the fact that $v_2$ is orthogonal to $v_1$.
            \begin{center}
                \wasysymLightning \marvosymLightning $\stmaryrdLightning$\wasysymLightning \marvosymLightning $\stmaryrdLightning$\wasysymLightning \marvosymLightning $\stmaryrdLightning$\wasysymLightning \marvosymLightning $\stmaryrdLightning$ CONTRADICTION REACHED \wasysymLightning \marvosymLightning $\stmaryrdLightning$ \wasysymLightning \marvosymLightning $\stmaryrdLightning$\wasysymLightning \marvosymLightning $\stmaryrdLightning$\wasysymLightning \marvosymLightning $\stmaryrdLightning$
            \end{center}
            As such, we have proved that the eigenvalue $0$ can not have more than one orthogonal eigenvector, so it's multiplicity for a individual connected graph can not be more than 1 (it is exactly one in fact because of point 2.).

\end{enumerate}
\end{document}